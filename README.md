**Description**

This project implements a fully connected neural network from scratch using NumPy
to classify handwritten digits from the MNIST dataset. The goal was to gain a deep
understanding of neural network internals, including forward propagation, backpropagation,
gradient descent, and activation functions without relying on deep learning frameworks.

**Requirement**

NumPy: For matrix operations 

Pandas: For loading the dataset 

Matplotlib: For visualizing digits

**Architecture**

Network Architecture:
- Input Layer: 784 neurons (28√ó28 image pixels)
- Hidden Layer 1: 72 neurons (ReLU)
- Hidden Layer 2: 36 neurons (ReLU)
- Output Layer: 10 neurons (Softmax)

**Key Concept Implemented** 

- Weight and bias initialization
- Forward propagation
- ReLU and Softmax activation functions
- Cross-entropy loss
- Backpropagation using chain rule
- Gradient descent optimization
- One-hot encoding of labels
- Model evaluation using accuracy

**MATH USED**

**1 Forward Propagation**
Z‚ÇÅ = W‚ÇÅX + b‚ÇÅ
A‚ÇÅ = ReLU(Z‚ÇÅ)

Z‚ÇÇ = W‚ÇÇA‚ÇÅ + b‚ÇÇ
A‚ÇÇ = ReLU(Z‚ÇÇ)

Z‚ÇÉ = W‚ÇÉA‚ÇÇ + b‚ÇÉ
A‚ÇÉ = Softmax(Z‚ÇÉ)



ùëå
Y = one-hot encoded true labels

This gradient represents the error signal at the output layer.


